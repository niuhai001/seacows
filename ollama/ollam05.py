import sys
from io import  StringIO
import logging
from logging.handlers import RotatingFileHandler
import streamlit as st
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableBranch, RunnablePassthrough,RunnableLambda
from langchain_community.embeddings import ZhipuAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_community.document_loaders import (
    PyPDFLoader,
    TextLoader,
    Docx2txtLoader,
    UnstructuredMarkdownLoader
)
import os
from langchain_text_splitters import RecursiveCharacterTextSplitter
from datetime import datetime 
from ollama import Client
from langchain_community.embeddings import OllamaEmbeddings


# åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
def setup_logging():
    log_dir = "./logs"
    os.makedirs(log_dir, exist_ok=True)
    
    logger = logging.getLogger()
    logger.setLevel(logging.INFO)
    
    # åˆ›å»ºæ–‡ä»¶å¤„ç†å™¨ï¼Œæ¯ä¸ªæ—¥å¿—æ–‡ä»¶æœ€å¤§10MBï¼Œä¿ç•™3ä¸ªå¤‡ä»½
    file_handler = RotatingFileHandler(
        os.path.join(log_dir, "app.log"),
        maxBytes=10*1024*1024,
        backupCount=3,
        encoding='utf-8'
    )
    file_handler.setFormatter(logging.Formatter(
        '%(asctime)s - %(levelname)s - %(message)s'
    ))
    
    # æ§åˆ¶å°å¤„ç†å™¨
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(logging.Formatter(
        '%(asctime)s - %(levelname)s - %(message)s'
    ))
    
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    return logger

logger = setup_logging()



def get_ollama_llm():
    try:
        client = Client(
            host='http://localhost:11434',
            headers={'x-some-header': 'some-value'}
        )
        logger.info("Ollamaå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        
        def ollama_chat(messages):
            try:
                logger.debug(f"å‘é€æ¶ˆæ¯åˆ°Ollama: {messages}")
                response = client.chat(
                    model='deepseek-r1:1.5b',
                    messages=messages,
                    stream=False 
                )
                logger.debug("ä»Ollamaæ”¶åˆ°å“åº”")
                return response['message']['content']
            except Exception as e:
                logger.error(f"OllamaèŠå¤©é”™è¯¯: {str(e)}")
                return "æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºç°é—®é¢˜"
        
        return ollama_chat
    except Exception as e:
        logger.error(f"åˆå§‹åŒ–Ollamaå®¢æˆ·ç«¯å¤±è´¥: {str(e)}")
        raise

def get_retriever():
    try:
        embedding = OllamaEmbeddings(
            model="mxbai-embed-large",
            base_url="http://localhost:11434"  
        )
        persist_directory = './data_base/vector_db/chroma'
        vectordb = Chroma(
            persist_directory=persist_directory,
            embedding_function=embedding
        )
        logger.info("å‘é‡æ£€ç´¢å™¨åˆå§‹åŒ–æˆåŠŸ")
        return vectordb.as_retriever()
    except Exception as e:
        logger.error(f"åˆå§‹åŒ–æ£€ç´¢å™¨å¤±è´¥: {str(e)}")
        raise


def gen_response(chain, input, chat_history,use_knowledge=True):
    response = chain({
        "input": input,
        "chat_history": chat_history,
        "use_knowledge": use_knowledge
    })
    yield response

def combine_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs["context"])

def process_uploaded_files(uploaded_files):
    """å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶å¹¶å­˜å…¥å‘é‡æ•°æ®åº“"""
    try:
        docs = []
        os.makedirs("./temp_uploads", exist_ok=True)
        logger.info(f"å¼€å§‹å¤„ç† {len(uploaded_files)} ä¸ªä¸Šä¼ æ–‡ä»¶")
        
        for file in uploaded_files:
            file_path = f"./temp_uploads/{file.name}"
            try:
                with open(file_path, "wb") as f:
                    f.write(file.getbuffer())
                
                loader = None
                if file.name.endswith(".pdf"):
                    loader = PyPDFLoader(file_path)
                elif file.name.endswith(".docx"):
                    loader = Docx2txtLoader(file_path)
                elif file.name.endswith(".txt"):
                    loader = TextLoader(file_path)
                elif file.name.endswith(".md"):
                    loader = UnstructuredMarkdownLoader(file_path)
                
                if loader:
                    docs.extend(loader.load())
                    logger.info(f"æˆåŠŸåŠ è½½æ–‡ä»¶: {file.name}")
                    os.remove(file_path)  # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
                    logger.info(f"åˆ é™¤ä¸´æ—¶æ–‡ä»¶: {file_path}")
            except Exception as e:
                logger.error(f"å¤„ç†æ–‡ä»¶ {file.name} æ—¶å‡ºé”™: {str(e)}")
        
        if docs:
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000,
                chunk_overlap=200
            )
            splits = text_splitter.split_documents(docs)
            
            embedding = OllamaEmbeddings(
                base_url="http://localhost:11434",
                model="mxbai-embed-large"
            )
            
            try:
                Chroma.from_documents(
                    documents=splits,
                    embedding=embedding,
                    persist_directory="./data_base/vector_db/chroma"
                )
                logger.info(f"æˆåŠŸå­˜å‚¨ {len(splits)} ä¸ªæ–‡æ¡£å—åˆ°å‘é‡æ•°æ®åº“")
            except Exception as e:
                logger.error(f"å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“å¤±è´¥: {str(e)}")
                raise
    except Exception as e:
        logger.error(f"å¤„ç†ä¸Šä¼ æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        raise

def get_qa_history_chain():
    try:
        retriever = get_retriever()
        llm = get_ollama_llm()

        knowledge_system_prompt = (
            "ä½ æ˜¯ä¸€ä¸ªé—®ç­”ä»»åŠ¡çš„åŠ©æ‰‹ã€‚è¯·ä¸¥æ ¼æ ¹æ®æä¾›çš„ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚\n"
            "ä¸Šä¸‹æ–‡ï¼š\n{context}\n\n"
            "é—®é¢˜ï¼š{input}\n"
            "å¦‚æœä¸Šä¸‹æ–‡ä¸åŒ…å«ç­”æ¡ˆï¼Œè¯·å›ç­”'æˆ‘ä¸çŸ¥é“'ã€‚"
        )
        
        general_system_prompt = (
            "ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½åŠ©æ‰‹ï¼Œè¯·ç›´æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚ä¸éœ€è¦å‚è€ƒç‰¹å®šæ–‡æ¡£ã€‚"
        )
        
        def run_ollama_chain(input_data):
            try:
                use_knowledge = input_data.get("use_knowledge", True)
                logger.info(f"ä½¿ç”¨çŸ¥è¯†åº“: {use_knowledge}, é—®é¢˜: {input_data['input']}")
                
                if use_knowledge:
                    # ä½¿ç”¨çŸ¥è¯†åº“æ£€ç´¢
                    retrieved_docs = retriever.get_relevant_documents(input_data["input"])
                    logger.debug(f"æ£€ç´¢åˆ° {len(retrieved_docs)} ä¸ªç›¸å…³æ–‡æ¡£")
                    
                    context = "\n\n---\n\n".join([doc.page_content for doc in retrieved_docs])
                    messages = [
                         {
                        "role": "system",
                        "content": knowledge_system_prompt.format(
                            context=context,
                            input=input_data["input"]
                        )
                        },
                        *[
                            {"role": "user" if msg[0] == "human" else "assistant", "content": msg[1]}
                            for msg in input_data.get("chat_history", [])
                        ],
                        {"role": "user", "content": input_data["input"]}
                    ]
                else:
                    # ä¸ä½¿ç”¨çŸ¥è¯†åº“
                    
                    messages = [
                        {
                            "role": "system",
                            "content": general_system_prompt.format(
                                context=input_data.get("context", ""),
                                input=input_data.get("input", "")
                        )
                        },
                        *[
                            {"role": "user" if msg[0] == "human" else "assistant", "content": msg[1]}
                            for msg in input_data.get("chat_history", [])
                        ],
                        {"role": "user", "content": input_data["input"]}
                    ]
                
                # æ·»åŠ èŠå¤©å†å²
                response = llm(messages)
                logger.debug("ç”Ÿæˆå›ç­”æˆåŠŸ")
                return response
            except Exception as e:
                logger.error(f"ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: {str(e)}")
                return "æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºç°é—®é¢˜"
        
        return run_ollama_chain
    except Exception as e:
        logger.error(f"åˆå§‹åŒ–é—®ç­”é“¾å¤±è´¥: {str(e)}")
        raise

def main():
    try:
        st.markdown('### ğŸ¦œğŸ”— åŠ¨æ‰‹å­¦å¤§æ¨¡å‹åº”ç”¨å¼€å‘')
        with st.sidebar:
            # æ·»åŠ çŸ¥è¯†åº“å¼€å…³
            use_knowledge = st.toggle(
                "ä½¿ç”¨çŸ¥è¯†åº“å›ç­”",
                value=True,
                help="å¯ç”¨åä¼šåŸºäºä¸Šä¼ çš„æ–‡ä»¶å†…å®¹å›ç­”ï¼Œç¦ç”¨åˆ™ä½¿ç”¨æ¨¡å‹çš„ä¸€èˆ¬çŸ¥è¯†"
            )
        with st.sidebar:
            uploaded_files = st.file_uploader(
                "ä¸Šä¼ çŸ¥è¯†åº“æ–‡ä»¶",
                type=["pdf", "txt", "docx", "md"],
                accept_multiple_files=True
            )
            if uploaded_files and st.button("å¤„ç†æ–‡ä»¶"):
                with st.spinner("æ­£åœ¨å¤„ç†æ–‡ä»¶..."):
                    process_uploaded_files(uploaded_files)
                    st.success("æ–‡ä»¶å¤„ç†å®Œæˆï¼")
        
        if "messages" not in st.session_state:
            st.session_state.messages = []
        
        if "qa_history_chain" not in st.session_state:
            st.session_state.qa_history_chain = get_qa_history_chain()
        
        messages = st.container(height=550)
        for message in st.session_state.messages:
            with messages.chat_message(message[0]):
                st.write(message[1])
        
        prompt = st.chat_input("Say something")
        if prompt:
            st.session_state.messages.append(("human", prompt))
            with messages.chat_message("human"):
                st.write(prompt)
            
            try:
                answer = gen_response(
                    chain=st.session_state.qa_history_chain,
                    input=prompt,
                    chat_history=st.session_state.messages,
                    use_knowledge=use_knowledge
                )
                with messages.chat_message("ai"):
                    output = st.write_stream(answer)
                st.session_state.messages.append(("ai", output))
            except Exception as e:
                logger.error(f"å¤„ç†ç”¨æˆ·è¾“å…¥æ—¶å‡ºé”™: {str(e)}")
                st.error("å¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºç°é—®é¢˜")
    except Exception as e:
        logger.critical(f"åº”ç”¨ç¨‹åºå´©æºƒ: {str(e)}")
        st.error("åº”ç”¨ç¨‹åºå‘ç”Ÿä¸¥é‡é”™è¯¯")

if __name__ == "__main__":
    main()
