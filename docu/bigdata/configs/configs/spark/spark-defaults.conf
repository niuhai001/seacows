#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

# Default system properties included when running spark-submit.
# This is useful for setting default environmental settings.

# Example:
spark.authenticate=false
spark.driver.log.dfsDir=/user/spark/driverLogs
spark.driver.log.persistToDfs.enabled=true
spark.dynamicAllocation.enabled=true
spark.dynamicAllocation.executorIdleTimeout=60
spark.dynamicAllocation.minExecutors=0
spark.dynamicAllocation.maxExecutors=20
spark.dynamicAllocation.schedulerBacklogTimeout=1
spark.eventLog.enabled=true
spark.io.encryption.enabled=false
spark.network.crypto.enabled=false
spark.serializer=org.apache.spark.serializer.KryoSerializer
spark.shuffle.service.enabled=true
spark.shuffle.service.port=7337
spark.shuffle.manager=sort
#spark.shuffle.manager=yarn
spark.shuffle.memoryFraction=0.4
spark.shuffle.spill=false
spark.shuffle.compress=true
spark.shuffle.compress.codec=lz4
spark.ui.enabled=true
spark.ui.killEnabled=true
spark.lineage.log.dir=/var/log/spark/lineage
spark.lineage.enabled=true
spark.master=yarn
spark.submit.deployMode=client
spark.yarn.historyServer.address=http://hadoop3:18088
spark.yarn.historyServer.allowTracking=true
spark.eventLog.dir=hdfs://hadoop3:2220/user/spark/applicationHistory
spark.executor.extraJavaOptions=-XX:+PrintGCDetails -Dkey=value -Dnumbers="one two three"
spark.yarn.containerLauncherMaxThreads=100
#spark.yarn.archive=hdfs://hadoop3:2220/user/spark/jars/spark-libs.jar
spark.yarn.appMasterEnv.MKL_NUM_THREADS=1
spark.executorEnv.MKL_NUM_THREADS=1
spark.yarn.appMasterEnv.OPENBLAS_NUM_THREADS=1
spark.executorEnv.OPENBLAS_NUM_THREADS=1
spark.driver.extraLibraryPath=/home/hadoop/core/lib/native
spark.executor.extraLibraryPath=/home/hadoop/core/lib/native/
spark.yarn.am.extraLibraryPath=/home/hadoop/core/lib/native/
spark.pyspark.python=/usr/bin/python3.7
spark.pyspark.driver.python=/usr/bin/python3.7
spark.sql.warehouse.dir=/user/hive/warehouse
#spark.sql.hive.metastore.jars=/home/hadoop/hive/lib/*
#spark.sql.hive.metastore.version=3.1.2
spark.jars.packages=org.apache.iceberg:iceberg-spark-runtime:0.13.1
spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
spark.sql.catalog.spark_catalog=org.apache.iceberg.spark.SparkSessionCatalog
spark.sql.catalog.spark_catalog.type=hive
spark.sql.catalog.local=org.apache.iceberg.spark.SparkCatalog
#spark.sql.catalog.local.type=hadoop
spark.sql.catalog.local.warehouse=/user/hive/warehouse
#spark.sql.defaultCatalog=local
spark.sql.catalogImplementation=hive


